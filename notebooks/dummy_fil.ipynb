{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy UNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    " \n",
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys ; sys.path.append('..')  # useful if you're running locally\n",
    "\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of local modules\n",
    "root = os.path.dirname(os.path.dirname(os.path.realpath('__file__')))\n",
    "sys.path.insert(0, os.path.join(root, 'src'))\n",
    "\n",
    "from dataset import dataset_dir\n",
    "from cs import CompressedSensing, generate_sensing_matrix\n",
    "from cs.utils import compute_rsnr\n",
    "from models.unet import UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "os.environ['SCIPY_USE_PROPACK'] = \"True\"\n",
    " \n",
    "threads = \"64\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = threads\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = threads\n",
    "os.environ[\"MKL_NUM_THREADS\"] = threads\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = threads\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10_000\n",
    "n = 128\n",
    "m = 48\n",
    "fs = 256\n",
    "hr = [60, 100]\n",
    "isnr = 35 # dB\n",
    "seed = 0\n",
    "\n",
    "supp_method = 'TSOC2'\n",
    "mode_A = 'standard'\n",
    "orth = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join('..', '..','newpenny','dnn-cs','tsoc','data')\n",
    "str_ecg_setting = f'ecg_N={N}_n={n}_fs={fs}_hr={hr[0]}-{hr[1]}_isnr={isnr}_seed={seed}'\n",
    "\n",
    "path_ecg = os.path.join(root, str_ecg_setting + '.pkl')\n",
    "path_s_GR = os.path.join(root, str_ecg_setting, 'supports_method=GR_eta=0.9999.pkl')\n",
    "path_s_standard = os.path.join(root, str_ecg_setting, f'supports_method={supp_method}_mode={mode_A}_m={m}_orth={orth}_seed={seed}.pkl')\n",
    "path_corr = os.path.join('..', '..','newpenny','dnn-cs','tsoc','data','correlation', '96af96a7ddfcb2f6059092c250e18f2a.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_ecg, 'rb') as f:\n",
    "    ecg = pkl.load(f)\n",
    "with open(path_s_standard, 'rb') as f:\n",
    "    s = pkl.load(f)\n",
    "with open(path_corr, 'rb') as f:\n",
    "    corr = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(ecg))\n",
    "val_size = int(0.1 * len(ecg))\n",
    "test_size = int(0.2 * len(ecg))\n",
    "train_dataset, val_dataset, test_dataset = random_split(ecg, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('mnist1d_data.pkl', 'rb') as f:\n",
    "#     dataset = pkl.load(f)\n",
    "#     x_train = dataset['x'][..., np.newaxis]\n",
    "#     x_test = dataset['x_test'][..., np.newaxis]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.normal(0, 1, (m, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_things(x, xbar_index=[1], return_x=True, return_y=True):\n",
    "\n",
    "    if x.shape[-1] != 1:\n",
    "        x = np.swapaxes(x, -1, -2)\n",
    "    y = A @ x\n",
    "\n",
    "    outputs = []\n",
    "    if return_x:\n",
    "        outputs += [x]\n",
    "    if return_y:\n",
    "        outputs += [y]\n",
    "\n",
    "    if 1 in xbar_index:\n",
    "        outputs += [A.T @ y]\n",
    "    if 2 in xbar_index:\n",
    "        xbar2 =+ [np.linalg.pinv(A) @ y]\n",
    "\n",
    "    if x.shape[-1] == 1:\n",
    "        outputs = [np.swapaxes(i, -1, -2) for i in outputs]\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 40, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (4000, 1, 40) \n",
      "y shape: (4000, 1, 15) \n",
      "xbar shape: (4000, 1, 40)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, xbar_train = derive_things(x_train)\n",
    "x_val, y_val, xbar_val = derive_things(x_val)\n",
    "x_test, y_test, xbar_test = derive_things(x_test)\n",
    "\n",
    "channels = x_train.shape[-2]\n",
    "print('x shape:', x_train.shape, '\\ny shape:', y_train.shape, '\\nxbar shape:', xbar_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_aug(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_input, train_output,mode='train'):\n",
    "        self.train_input = train_input\n",
    "        self.train_output = train_output\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_input)\n",
    "    \n",
    "    def _augmentations(self,input_data, target_data):\n",
    "        #flip\n",
    "        if np.random.rand()<0.5:    \n",
    "            input_data = input_data[::-1]\n",
    "            target_data = target_data[::-1]\n",
    "        return input_data, target_data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.train_input[idx]\n",
    "        y = self.train_output[idx]\n",
    "        if self.mode =='train_aug':\n",
    "            x,y = self._augmentations(x,y)\n",
    "\n",
    "        out_y = torch.tensor(y.copy(), dtype=torch.float)\n",
    "        out_x = torch.tensor(x.copy(), dtype=torch.float)\n",
    "        \n",
    "        # out_x = torch.tensor(np.transpose(x.copy(),(1,0)), dtype=torch.float)\n",
    "        # out_y = torch.tensor(np.transpose(y.copy(),(1,0)), dtype=torch.float)\n",
    "        return out_x, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train = Dataset_aug(xbar_train, x_train, mode='train')\n",
    "valid = Dataset_aug(xbar_val, x_val, mode='valid')\n",
    "test = Dataset_aug(xbar_test, x_test, mode='test')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_patience = 20\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "cuda_index = torch.cuda.device_count() - 1\n",
    "device = torch.device(f\"cuda:{cuda_index}\" if use_cuda else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_channels=channels, num_classes=channels,\n",
    "                  channels=8, steps_num=2, kernel_size=3, residual=False,\n",
    "                  use_batch_norm=False, simple_pool=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "for inputs, targets in train_loader:\n",
    "    \n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "    outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 40]), torch.Size([32, 1, 40]), torch.Size([32, 1, 40]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape, targets.shape, inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # calculate validation loss\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_loss_list = []\n",
    "    val_correct_predictions = 0\n",
    "    val_total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, targets = data\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_loss_list += [val_loss]\n",
    "\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    #print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, ')\n",
    "        #   f'Train SNR: {train_SNR:.2f}%, Val Accuracy: {val_SNR:.2f}%')\n",
    "    \n",
    "    # early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping: Validation loss hasn't improved for\", early_stopping_patience, \"epochs.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0dad154c70>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter\n",
    "n_epochs = 100\n",
    "lr = 0.001\n",
    "\n",
    "##Loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "#Build model, initial weight and optimizer\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr,weight_decay=1e-5) # Using Adam optimizer\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.2, min_lr=1e-8) # Using ReduceLROnPlateau schedule\n",
    "temp_val_loss = 1e5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        y_pred = model(x_batch.cuda())\n",
    "        \n",
    "        loss = loss_fn(y_pred.cpu(), y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()/len(train_loader)\n",
    "\n",
    "        # pred = F.softmax(y_pred, 1).detach().cpu().numpy().argmax(axis=1)\n",
    "        # train_preds[i * batch_size*train_input.shape[1]:(i+1) * batch_size*train_input.shape[1]] = pred.reshape((-1))\n",
    "        # train_targets[i * batch_size*train_input.shape[1]:(i+1) * batch_size*train_input.shape[1]] = y_batch.detach().cpu().argmax(axis=1).reshape((-1))\n",
    "        del y_pred, loss, x_batch, y_batch, pred\n",
    "        \n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    avg_val_loss = 0.\n",
    "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "        y_pred = model(x_batch.cuda()).detach()\n",
    "\n",
    "        avg_val_loss += loss_fn(y_pred.cpu(), y_batch).item() / len(valid_loader)\n",
    "        pred = F.softmax(y_pred, 1).detach().cpu().numpy().argmax(axis=1)\n",
    "        val_preds[i * batch_size*val_input.shape[1]:(i+1) * batch_size*val_input.shape[1]] = pred.reshape((-1))\n",
    "        del y_pred, x_batch, y_batch, pred\n",
    "        \n",
    "    if avg_val_loss<temp_val_loss:\n",
    "        #print ('checkpoint_save')\n",
    "        temp_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'ION_train_checkpoint.pt')\n",
    "        \n",
    "    train_score = f1_score(train_targets,train_preds,average = 'macro')\n",
    "    val_score = f1_score(val_target.argmax(axis=2).reshape((-1)),val_preds,average = 'macro')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time \n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    print('Epoch {}/{} \\t loss={:.4f} \\t train_f1={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} \\t time={:.2f}s'.format(\n",
    "        epoch + 1, n_epochs, avg_loss,train_score, avg_val_loss,val_score, elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape torch.Size([1, 3, 128])\n"
     ]
    }
   ],
   "source": [
    "n = 128\n",
    "\n",
    "inp_shape = (1, 3, n)\n",
    "\n",
    "input_image = torch.rand(inp_shape)\n",
    "\n",
    "\n",
    "output = model(input_image)\n",
    "print('output shape', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((1, 2, 4))\n",
    "padding, out_padding = 0, 0\n",
    "dilation = 1\n",
    "nn.Conv1d(\n",
    "    in_channels=2,\n",
    "    out_channels=4,\n",
    "    kernel_size=2,\n",
    "    stride=2)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 8, 8])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((1, 2, 4, 4))\n",
    "padding, out_padding = 0, 0\n",
    "dilation = 1\n",
    "nn.ConvTranspose2d(\n",
    "    in_channels=2,\n",
    "    out_channels=2, \n",
    "    padding=padding, \n",
    "    dilation=dilation, \n",
    "    kernel_size=2, \n",
    "    stride=2,\n",
    "    output_padding=out_padding)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 3, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels, out_channels = 1, 4\n",
    "kernel_size = (1, 3)\n",
    "padding_size = (1, 1)\n",
    "padding_size = 1\n",
    "\n",
    "lconv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding_size)\n",
    "lconv(x).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
